{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuroAlign - Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  2  GPU devices.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "import Model as model\n",
    "import Data as data\n",
    "\n",
    "\n",
    "GPUS = tf.config.experimental.list_logical_devices('GPU')\n",
    "NUM_DEVICES = max(1, len(GPUS))\n",
    "\n",
    "if len(GPUS) > 0:\n",
    "    print(\"Using \", NUM_DEVICES, \" GPU devices.\")\n",
    "else:\n",
    "    print(\"Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured model  base2  and initialized weights randomly.\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 200\n",
    "NAME = \"base2\"\n",
    "MODEL_PATH = \"./models/\" + NAME\n",
    "CHECKPOINT_PATH = MODEL_PATH + \"/model.ckpt\"\n",
    "\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "neuroalign, neuroalign_config = model.make_neuro_align_model(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 % loaded\n",
      "20 % loaded\n",
      "30 % loaded\n",
      "40 % loaded\n",
      "50 % loaded\n",
      "60 % loaded\n",
      "70 % loaded\n",
      "80 % loaded\n",
      "90 % loaded\n",
      "Using the full dataset.\n"
     ]
    }
   ],
   "source": [
    "#Pfam protein families have identifiers of the form PF00001, PF00002, ...\n",
    "#The largest id is PF19227, but the counting is not contiguous, there may be missing numbers\n",
    "pfam = [\"PF\"+\"{0:0=5d}\".format(i) for i in range(1,19228)]\n",
    "pfam_not_found = 0\n",
    "\n",
    "fasta = []\n",
    "\n",
    "for i,file in enumerate(pfam):\n",
    "    try:\n",
    "        f = data.Fasta(\"../brain/Pfam/alignments/\" + file + \".fasta\", gaps = True, contains_lower_case = True)\n",
    "        fasta.append(f)\n",
    "        for x in range(1,10):\n",
    "            if i/len(pfam) > x/10 and (i-1)/len(pfam) < x/10:\n",
    "                print(x*10, \"% loaded\")\n",
    "                gc.collect()\n",
    "    except:\n",
    "        pfam_not_found += 1\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "indices = np.arange(len(fasta))\n",
    "np.random.shuffle(indices)\n",
    "if len(fasta) > 10:\n",
    "    print(\"Using the full dataset.\")\n",
    "    train, val = np.split(indices, [int(len(fasta)*(1-neuroalign_config[\"validation_split\"]))]) \n",
    "    train_gen = data.AlignmentSampleGenerator(train, fasta, neuroalign_config, neuroalign_config[\"family_size\"], NUM_DEVICES)\n",
    "    val_gen = data.AlignmentSampleGenerator(val, fasta, neuroalign_config, 2*neuroalign_config[\"family_size\"], NUM_DEVICES, False)\n",
    "else: \n",
    "    print(\"Using a small test dataset.\")\n",
    "    train_gen = data.AlignmentSampleGenerator(np.arange(len(fasta)), fasta, neuroalign_config, neuroalign_config[\"family_size\"], NUM_DEVICES)\n",
    "    val_gen = data.AlignmentSampleGenerator(np.arange(len(fasta)), fasta, neuroalign_config, 2*neuroalign_config[\"family_size\"], NUM_DEVICES, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17346/17346 [==============================] - ETA: 0s - loss: 0.0999 - GPU_0_out_columns_loss: 1.4899 - GPU_0_out_attention_loss: 0.0176 - GPU_1_out_columns_loss: 1.4528 - GPU_1_out_attention_loss: 0.0243Saved model to ./models/base2/model.ckpt\n",
      "17346/17346 [==============================] - 2117s 122ms/step - loss: 0.0999 - GPU_0_out_columns_loss: 1.4899 - GPU_0_out_attention_loss: 0.0176 - GPU_1_out_columns_loss: 1.4528 - GPU_1_out_attention_loss: 0.0243 - val_loss: 0.0863 - val_GPU_0_out_columns_loss: 1.2567 - val_GPU_0_out_attention_loss: 0.0164 - val_GPU_1_out_columns_loss: 1.1811 - val_GPU_1_out_attention_loss: 0.0220\n",
      "Epoch 2/200\n",
      "17346/17346 [==============================] - ETA: 0s - loss: 0.0863 - GPU_0_out_columns_loss: 1.3287 - GPU_0_out_attention_loss: 0.0156 - GPU_1_out_columns_loss: 1.2103 - GPU_1_out_attention_loss: 0.0207Saved model to ./models/base2/model.ckpt\n",
      "17346/17346 [==============================] - 2123s 122ms/step - loss: 0.0863 - GPU_0_out_columns_loss: 1.3287 - GPU_0_out_attention_loss: 0.0156 - GPU_1_out_columns_loss: 1.2103 - GPU_1_out_attention_loss: 0.0207 - val_loss: 0.0772 - val_GPU_0_out_columns_loss: 1.1472 - val_GPU_0_out_attention_loss: 0.0149 - val_GPU_1_out_columns_loss: 1.0176 - val_GPU_1_out_attention_loss: 0.0198\n",
      "Epoch 3/200\n",
      " 6554/17346 [==========>...................] - ETA: 20:54 - loss: 0.0791 - GPU_0_out_columns_loss: 1.1982 - GPU_0_out_attention_loss: 0.0151 - GPU_1_out_columns_loss: 1.0460 - GPU_1_out_attention_loss: 0.0199"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = 28\n",
    "\n",
    "COLUMN_LOSS_WEIGHT = 0.02\n",
    "ATTENTION_LOSS_WEIGHT = 0.98\n",
    "SEQUENCE_LOSS_WEIGHT = 1\n",
    "\n",
    "POS_WEIGHT = 1\n",
    "NEG_WEIGHT = 1\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(neuroalign_config[\"col_dim\"], tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(CustomSchedule(), beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "#loss for aligned aminoacid pairs (= attention)\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "     \n",
    "def make_mask(y_true):\n",
    "    mask = tf.math.equal(y_true, 0)\n",
    "    mask = tf.math.reduce_all(mask, axis=-1)\n",
    "    mask = tf.cast(tf.math.logical_not(mask), y_true.dtype)\n",
    "    return mask\n",
    "\n",
    "def make_sq(y, mask):\n",
    "    y = tf.boolean_mask(y, mask)\n",
    "    y_sq = tf.matmul(y, y, transpose_b=True)\n",
    "    y_sq = tf.reshape(y_sq, (-1, 1))\n",
    "    y_sq = tf.clip_by_value(y_sq, 0.0, 1.0)\n",
    "    return y_sq\n",
    "\n",
    "def att_loss(y_true, y_pred):\n",
    "    mask = make_mask(y_true)\n",
    "    y_true_sq = make_sq(y_true, mask)\n",
    "    y_pred_sq = make_sq(y_pred, mask)\n",
    "    l = tf.expand_dims(bce(y_true_sq, y_pred_sq), -1)\n",
    "    w = POS_WEIGHT * y_true_sq + NEG_WEIGHT * (1-y_true_sq)\n",
    "    l *= w\n",
    "    return tf.reduce_sum(l) / tf.reduce_sum(w)\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "#loss for sequence reconstruction from columns (unsupervised)\n",
    "\n",
    "ce = keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "def seq_loss(y_true, y_pred):\n",
    "    mask = make_mask(y_true)\n",
    "    l = ce(y_true, y_pred) * mask\n",
    "    return tf.math.reduce_sum(l) / tf.math.reduce_sum(mask)\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "#precision and recall metrics for aligned aminoacid pairs\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    mask = make_mask(y_true)\n",
    "    y_true_sq = make_sq(y_true, mask)\n",
    "    y_pred_sq = make_sq(y_pred, mask)\n",
    "    positives = tf.cast(y_pred_sq >= threshold, tf.float32) \n",
    "    true_positives = positives * y_true_sq\n",
    "    precision = tf.reduce_sum(true_positives) / tf.math.maximum(tf.reduce_sum(positives), 1.0)\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    mask = make_mask(y_true)\n",
    "    y_true_sq = make_sq(y_true, mask)\n",
    "    y_pred_sq = make_sq(y_pred, mask)\n",
    "    positives = tf.cast(y_pred_sq >= threshold, tf.float32)\n",
    "    true_positives = positives * y_true_sq\n",
    "    recall = tf.reduce_sum(true_positives) / tf.math.maximum(tf.reduce_sum(y_true_sq), 1.0)\n",
    "    return recall\n",
    "\n",
    "#categorical accuracy for reconstructed sequences\n",
    "\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    mask = make_mask(y_true)\n",
    "    acc = tf.equal(tf.argmax(y_true, axis=-1), tf.argmax(y_pred, axis=-1)) \n",
    "    acc = tf.cast(acc, dtype=y_true.dtype)\n",
    "    return tf.math.reduce_sum(acc * mask) / tf.math.reduce_sum(mask)\n",
    "    \n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "def losses_prefixed(losses, metrics, weights, prefix=\"\"):\n",
    "    if neuroalign_config[\"use_column_loss\"]:\n",
    "        losses.update({prefix+\"out_columns\" : tf.keras.losses.KLDivergence()})\n",
    "        weights.update({prefix+\"out_columns\" : COLUMN_LOSS_WEIGHT})\n",
    "    if neuroalign_config[\"use_attention_loss\"]:\n",
    "        losses.update({prefix+\"out_attention\" : att_loss})\n",
    "        metrics.update({prefix+\"out_attention\" : []})\n",
    "        weights.update({prefix+\"out_attention\" : ATTENTION_LOSS_WEIGHT})\n",
    "        \n",
    "\n",
    "losses, metrics, weights = {}, {}, {}\n",
    "if NUM_DEVICES == 1:\n",
    "    model = neuroalign\n",
    "    losses_prefixed(losses, metrics, weights)\n",
    "else:\n",
    "    inputs, outputs = [], []\n",
    "    for i, gpu in enumerate(GPUS):\n",
    "        with tf.device(gpu.name):\n",
    "            sequences = keras.Input(shape=(None,INPUT_DIM), name=\"GPU_\"+str(i)+\"_sequences\")\n",
    "            columns = keras.Input(shape=(INPUT_DIM), name=\"GPU_\"+str(i)+\"_in_columns\")\n",
    "            input_dict = {  \"sequences\" : sequences,\n",
    "                            \"in_columns\" : columns }\n",
    "            out_cols, A = neuroalign(input_dict)\n",
    "            outputs.append(layers.Lambda(lambda x: x, name=\"GPU_\"+str(i)+\"_out_columns\")(out_cols))\n",
    "            outputs.append(layers.Lambda(lambda x: x, name=\"GPU_\"+str(i)+\"_out_attention\")(A))\n",
    "            inputs.extend([sequences, columns])\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    for i, gpu in enumerate(GPUS):\n",
    "        losses_prefixed(losses, metrics, weights, \"GPU_\"+str(i)+\"_\")\n",
    "\n",
    "model.compile(loss=losses, optimizer=optimizer, metrics=metrics, loss_weights=weights)\n",
    "    \n",
    "class ModelCheckpoint(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        neuroalign.save_weights(CHECKPOINT_PATH)\n",
    "        print(\"Saved model to \" + CHECKPOINT_PATH, flush=True)\n",
    "\n",
    "csv_logger = CSVLogger(MODEL_PATH + \"/log.csv\", append=True, separator=',')\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    validation_data=val_gen,\n",
    "                    epochs = NUM_EPOCHS,\n",
    "                    verbose = 1,\n",
    "                    callbacks=[ModelCheckpoint(), csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
