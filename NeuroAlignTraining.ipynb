{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuroAlign - Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_probability==0.11.0 in /opt/conda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow_probability==0.11.0) (1.18.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from tensorflow_probability==0.11.0) (4.4.2)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.8/site-packages (from tensorflow_probability==0.11.0) (0.1.5)\n",
      "Requirement already satisfied: gast>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow_probability==0.11.0) (0.3.3)\n",
      "Requirement already satisfied: cloudpickle==1.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow_probability==0.11.0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow_probability==0.11.0) (1.15.0)\n",
      "Using  2  GPU devices.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow_probability==0.11.0\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "import Model as model\n",
    "import Data as data\n",
    "import Evaluation as eval\n",
    "\n",
    "GPUS = tf.config.experimental.list_logical_devices('GPU')\n",
    "NUM_DEVICES = max(1, len(GPUS))\n",
    "\n",
    "if len(GPUS) > 0:\n",
    "    print(\"Using \", NUM_DEVICES, \" GPU devices.\")\n",
    "else:\n",
    "    print(\"Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured model gap_prob and initialized weights randomly.\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 200\n",
    "NAME = \"gap_prob\"\n",
    "MODEL_PATH = \"./models/\" + NAME\n",
    "CHECKPOINT_PATH = MODEL_PATH + \"/model.ckpt\"\n",
    "\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "neuroalign, neuroalign_config = model.make_neuro_align_model(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 % loaded\n",
      "20 % loaded\n",
      "30 % loaded\n",
      "40 % loaded\n",
      "50 % loaded\n",
      "60 % loaded\n",
      "70 % loaded\n",
      "80 % loaded\n",
      "90 % loaded\n",
      "Using the full dataset.\n"
     ]
    }
   ],
   "source": [
    "#Pfam protein families have identifiers of the form PF00001, PF00002, ...\n",
    "#The largest id is PF19227, but the counting is not contiguous, there may be missing numbers\n",
    "pfam = [\"PF\"+\"{0:0=5d}\".format(i) for i in range(1,19228)]\n",
    "pfam_not_found = 0\n",
    "\n",
    "fasta = []\n",
    "\n",
    "for i,file in enumerate(pfam):\n",
    "    try:\n",
    "        f = data.Fasta(\"../brain/Pfam/alignments/\" + file + \".fasta\", gaps = True, contains_lower_case = True)\n",
    "        fasta.append(f)\n",
    "        for x in range(1,10):\n",
    "            if i/len(pfam) > x/10 and (i-1)/len(pfam) < x/10:\n",
    "                print(x*10, \"% loaded\")\n",
    "                gc.collect()\n",
    "    except:\n",
    "        pfam_not_found += 1\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "indices = np.arange(len(fasta))\n",
    "np.random.shuffle(indices)\n",
    "if len(fasta) > 1000:\n",
    "    print(\"Using the full dataset.\")\n",
    "    train, val = np.split(indices, [int(len(fasta)*(1-neuroalign_config[\"validation_split\"]))]) \n",
    "    train_gen = data.AlignmentSampleGenerator(train, fasta, neuroalign_config, neuroalign_config[\"family_size\"], NUM_DEVICES)\n",
    "    val_gen = data.AlignmentSampleGenerator(val, fasta, neuroalign_config, neuroalign_config[\"family_size\"], NUM_DEVICES, False)\n",
    "else: \n",
    "    print(\"Using a small test dataset.\")\n",
    "    train_gen = data.AlignmentSampleGenerator(np.arange(len(fasta)), fasta, neuroalign_config, neuroalign_config[\"family_size\"], NUM_DEVICES)\n",
    "    val_gen = data.AlignmentSampleGenerator(np.arange(len(fasta)), fasta, neuroalign_config, neuroalign_config[\"family_size\"], NUM_DEVICES, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17346/17346 [==============================] - ETA: 0s - loss: 1.1463 - GPU_0_out_gaps_loss: 0.5893 - GPU_1_out_gaps_loss: 0.5571 - GPU_0_out_gaps_categorical_accuracy: 0.8913 - GPU_1_out_gaps_categorical_accuracy: 0.8901Saved model to ./models/gap_prob/model.ckpt\n",
      "17346/17346 [==============================] - 1384s 80ms/step - loss: 1.1463 - GPU_0_out_gaps_loss: 0.5893 - GPU_1_out_gaps_loss: 0.5571 - GPU_0_out_gaps_categorical_accuracy: 0.8913 - GPU_1_out_gaps_categorical_accuracy: 0.8901 - val_loss: 1.1272 - val_GPU_0_out_gaps_loss: 0.5761 - val_GPU_1_out_gaps_loss: 0.5511 - val_GPU_0_out_gaps_categorical_accuracy: 0.8894 - val_GPU_1_out_gaps_categorical_accuracy: 0.8901\n",
      "Epoch 2/200\n",
      "  620/17346 [>.............................] - ETA: 21:54 - loss: 1.1335 - GPU_0_out_gaps_loss: 0.5855 - GPU_1_out_gaps_loss: 0.5480 - GPU_0_out_gaps_categorical_accuracy: 0.8928 - GPU_1_out_gaps_categorical_accuracy: 0.8934"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = 27\n",
    "\n",
    "#COLUMN_LOSS_WEIGHT = 0.02\n",
    "#ATTENTION_LOSS_WEIGHT = 0.98\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(neuroalign_config[\"col_dim\"], tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "def losses_prefixed(losses, metrics, weights, prefix=\"\"):\n",
    "    #if neuroalign_config[\"use_column_loss\"]:\n",
    "        #losses.update({prefix+\"out_columns\" : eval.kld})\n",
    "        #weights.update({prefix+\"out_columns\" : COLUMN_LOSS_WEIGHT})\n",
    "    #if neuroalign_config[\"use_attention_loss\"]:\n",
    "        #losses.update({prefix+\"out_attention\" : eval.att_loss})\n",
    "        #metrics.update({prefix+\"out_attention\" : [eval.precision, eval.recall]})\n",
    "        #weights.update({prefix+\"out_attention\" : ATTENTION_LOSS_WEIGHT})\n",
    "    losses.update({prefix+\"out_gaps\" : \n",
    "                   keras.losses.CategoricalCrossentropy(\n",
    "                       label_smoothing=0.1)})\n",
    "    metrics.update({prefix+\"out_gaps\" : keras.metrics.CategoricalAccuracy()})\n",
    "        \n",
    "\n",
    "losses, metrics, weights = {}, {}, {}\n",
    "if NUM_DEVICES == 1:\n",
    "    model = neuroalign\n",
    "    losses_prefixed(losses, metrics, weights)\n",
    "else:\n",
    "    inputs, outputs = [], []\n",
    "    for i, gpu in enumerate(GPUS):\n",
    "        with tf.device(gpu.name):\n",
    "            sequences = keras.Input(shape=(None,INPUT_DIM), name=\"GPU_\"+str(i)+\"_sequences\")\n",
    "            in_gaps = keras.Input(shape=(None,4), name=\"GPU_\"+str(i)+\"_in_gaps\")\n",
    "            input_dict = {  \"sequences\" : sequences,\n",
    "                            \"in_gaps\" : in_gaps }\n",
    "            #out_cols, A = neuroalign(input_dict)\n",
    "            out_gaps = neuroalign(input_dict)\n",
    "            outputs.append(layers.Lambda(lambda x: x, name=\"GPU_\"+str(i)+\"_out_gaps\")(out_gaps))\n",
    "            #outputs.append(layers.Lambda(lambda x: x, name=\"GPU_\"+str(i)+\"_out_attention\")(A))\n",
    "            inputs.extend([sequences, in_gaps])\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    for i, gpu in enumerate(GPUS):\n",
    "        losses_prefixed(losses, metrics, weights, \"GPU_\"+str(i)+\"_\")\n",
    "\n",
    "model.compile(loss=losses, optimizer=optimizer, metrics=metrics, loss_weights=weights)\n",
    "    \n",
    "class ModelCheckpoint(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        neuroalign.save_weights(CHECKPOINT_PATH)\n",
    "        print(\"Saved model to \" + CHECKPOINT_PATH, flush=True)\n",
    "\n",
    "csv_logger = CSVLogger(MODEL_PATH + \"/log.csv\", append=True, separator=',')\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    validation_data=val_gen,\n",
    "                    epochs = NUM_EPOCHS,\n",
    "                    verbose = 1,\n",
    "                    callbacks=[ModelCheckpoint(), csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
