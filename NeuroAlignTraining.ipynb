{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuroAlign - Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  2  GPU devices.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "import Model as model\n",
    "import Data as data\n",
    "import Evaluation as eval\n",
    "\n",
    "GPUS = tf.config.experimental.list_logical_devices('GPU')\n",
    "NUM_DEVICES = max(1, len(GPUS))\n",
    "\n",
    "if len(GPUS) > 0:\n",
    "    print(\"Using \", NUM_DEVICES, \" GPU devices.\")\n",
    "else:\n",
    "    print(\"Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded weights: base2 model\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 200\n",
    "NAME = \"base2\"\n",
    "MODEL_PATH = \"./models/\" + NAME\n",
    "CHECKPOINT_PATH = MODEL_PATH + \"/model.ckpt\"\n",
    "\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "neuroalign, neuroalign_config = model.make_neuro_align_model(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 % loaded\n",
      "20 % loaded\n",
      "30 % loaded\n",
      "40 % loaded\n",
      "50 % loaded\n",
      "60 % loaded\n",
      "70 % loaded\n",
      "80 % loaded\n",
      "90 % loaded\n",
      "Using the full dataset.\n"
     ]
    }
   ],
   "source": [
    "#Pfam protein families have identifiers of the form PF00001, PF00002, ...\n",
    "#The largest id is PF19227, but the counting is not contiguous, there may be missing numbers\n",
    "pfam = [\"PF\"+\"{0:0=5d}\".format(i) for i in range(1,19228)]\n",
    "pfam_not_found = 0\n",
    "#pfam = pfam[:1]\n",
    "fasta = []\n",
    "\n",
    "for i,file in enumerate(pfam):\n",
    "    try:\n",
    "        f = data.Fasta(\"../brain/Pfam/alignments/\" + file + \".fasta\", gaps = True, contains_lower_case = True)\n",
    "        fasta.append(f)\n",
    "        for x in range(1,10):\n",
    "            if i/len(pfam) > x/10 and (i-1)/len(pfam) < x/10:\n",
    "                print(x*10, \"% loaded\")\n",
    "                gc.collect()\n",
    "    except:\n",
    "        pfam_not_found += 1\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "indices = np.arange(len(fasta))\n",
    "np.random.shuffle(indices)\n",
    "if len(fasta) > 10:\n",
    "    print(\"Using the full dataset.\")\n",
    "    train, val = np.split(indices, [int(len(fasta)*(1-neuroalign_config[\"validation_split\"]))]) \n",
    "    train_gen = data.AlignmentSampleGenerator(train, fasta, neuroalign_config, neuroalign_config[\"family_size\"], NUM_DEVICES)\n",
    "    val_gen = data.AlignmentSampleGenerator(val, fasta, neuroalign_config, 2*neuroalign_config[\"family_size\"], NUM_DEVICES, False)\n",
    "else: \n",
    "    print(\"Using a small test dataset.\")\n",
    "    train_gen = data.AlignmentSampleGenerator(np.arange(len(fasta)), fasta, neuroalign_config, neuroalign_config[\"family_size\"], NUM_DEVICES)\n",
    "    val_gen = data.AlignmentSampleGenerator(np.arange(len(fasta)), fasta, neuroalign_config, 2*neuroalign_config[\"family_size\"], NUM_DEVICES, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17346/17346 [==============================] - ETA: 0s - loss: 1.2443 - GPU_0_out_columns_loss: 0.4139 - GPU_0_out_attention_loss: 0.7157 - GPU_1_out_columns_loss: 0.3759 - GPU_1_out_attention_loss: 0.6422 - GPU_0_out_columns_categorical_accuracy: 0.7746 - GPU_0_out_attention_precision: 0.8705 - GPU_0_out_attention_recall: 0.6961 - GPU_1_out_columns_categorical_accuracy: 0.7742 - GPU_1_out_attention_precision: 0.8695 - GPU_1_out_attention_recall: 0.6943Saved model to ./models/base2/model.ckpt\n",
      "17346/17346 [==============================] - 3522s 203ms/step - loss: 1.2443 - GPU_0_out_columns_loss: 0.4139 - GPU_0_out_attention_loss: 0.7157 - GPU_1_out_columns_loss: 0.3759 - GPU_1_out_attention_loss: 0.6422 - GPU_0_out_columns_categorical_accuracy: 0.7746 - GPU_0_out_attention_precision: 0.8705 - GPU_0_out_attention_recall: 0.6961 - GPU_1_out_columns_categorical_accuracy: 0.7742 - GPU_1_out_attention_precision: 0.8695 - GPU_1_out_attention_recall: 0.6943 - val_loss: 1.2791 - val_GPU_0_out_columns_loss: 0.3935 - val_GPU_0_out_attention_loss: 0.7841 - val_GPU_1_out_columns_loss: 0.3427 - val_GPU_1_out_attention_loss: 0.6307 - val_GPU_0_out_columns_categorical_accuracy: 0.7917 - val_GPU_0_out_attention_precision: 0.8609 - val_GPU_0_out_attention_recall: 0.7373 - val_GPU_1_out_columns_categorical_accuracy: 0.7886 - val_GPU_1_out_attention_precision: 0.8645 - val_GPU_1_out_attention_recall: 0.7413\n",
      "Epoch 2/200\n",
      "17346/17346 [==============================] - ETA: 0s - loss: 1.1590 - GPU_0_out_columns_loss: 0.3898 - GPU_0_out_attention_loss: 0.6635 - GPU_1_out_columns_loss: 0.3564 - GPU_1_out_attention_loss: 0.5987 - GPU_0_out_columns_categorical_accuracy: 0.7797 - GPU_0_out_attention_precision: 0.8737 - GPU_0_out_attention_recall: 0.7125 - GPU_1_out_columns_categorical_accuracy: 0.7807 - GPU_1_out_attention_precision: 0.8736 - GPU_1_out_attention_recall: 0.7136Saved model to ./models/base2/model.ckpt\n",
      "17346/17346 [==============================] - 3535s 204ms/step - loss: 1.1590 - GPU_0_out_columns_loss: 0.3898 - GPU_0_out_attention_loss: 0.6635 - GPU_1_out_columns_loss: 0.3564 - GPU_1_out_attention_loss: 0.5987 - GPU_0_out_columns_categorical_accuracy: 0.7797 - GPU_0_out_attention_precision: 0.8737 - GPU_0_out_attention_recall: 0.7125 - GPU_1_out_columns_categorical_accuracy: 0.7807 - GPU_1_out_attention_precision: 0.8736 - GPU_1_out_attention_recall: 0.7136 - val_loss: 1.2612 - val_GPU_0_out_columns_loss: 0.3857 - val_GPU_0_out_attention_loss: 0.7622 - val_GPU_1_out_columns_loss: 0.3415 - val_GPU_1_out_attention_loss: 0.6325 - val_GPU_0_out_columns_categorical_accuracy: 0.7793 - val_GPU_0_out_attention_precision: 0.8723 - val_GPU_0_out_attention_recall: 0.7461 - val_GPU_1_out_columns_categorical_accuracy: 0.7771 - val_GPU_1_out_attention_precision: 0.8634 - val_GPU_1_out_attention_recall: 0.7329\n",
      "Epoch 3/200\n",
      "  198/17346 [..............................] - ETA: 57:01 - loss: 1.1479 - GPU_0_out_columns_loss: 0.3935 - GPU_0_out_attention_loss: 0.6489 - GPU_1_out_columns_loss: 0.3501 - GPU_1_out_attention_loss: 0.6001 - GPU_0_out_columns_categorical_accuracy: 0.7762 - GPU_0_out_attention_precision: 0.8710 - GPU_0_out_attention_recall: 0.7128 - GPU_1_out_columns_categorical_accuracy: 0.7759 - GPU_1_out_attention_precision: 0.8707 - GPU_1_out_attention_recall: 0.7161"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = 28\n",
    "\n",
    "COLUMN_LOSS_WEIGHT = 0.2\n",
    "ATTENTION_LOSS_WEIGHT = 0.8\n",
    "SEQUENCE_LOSS_WEIGHT = 1\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(neuroalign_config[\"col_dim\"], tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(CustomSchedule(), beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "def losses_prefixed(losses, metrics, weights, prefix=\"\"):\n",
    "    if neuroalign_config[\"use_column_loss\"]:\n",
    "        losses.update({prefix+\"out_columns\" : eval.kld})\n",
    "        metrics.update({prefix+\"out_columns\" : \"categorical_accuracy\"})\n",
    "        weights.update({prefix+\"out_columns\" : COLUMN_LOSS_WEIGHT})\n",
    "    if neuroalign_config[\"use_attention_loss\"]:\n",
    "        losses.update({prefix+\"out_attention\" : \"categorical_crossentropy\"})\n",
    "        metrics.update({prefix+\"out_attention\" : [eval.precision, eval.recall]})\n",
    "        weights.update({prefix+\"out_attention\" : ATTENTION_LOSS_WEIGHT})\n",
    "        \n",
    "\n",
    "losses, metrics, weights = {}, {}, {}\n",
    "if NUM_DEVICES == 1:\n",
    "    model = neuroalign\n",
    "    losses_prefixed(losses, metrics, weights)\n",
    "else:\n",
    "    inputs, outputs = [], []\n",
    "    for i, gpu in enumerate(GPUS):\n",
    "        with tf.device(gpu.name):\n",
    "            sequences = keras.Input(shape=(None,INPUT_DIM), name=\"GPU_\"+str(i)+\"_sequences\")\n",
    "            columns = keras.Input(shape=(INPUT_DIM), name=\"GPU_\"+str(i)+\"_in_columns\")\n",
    "            input_dict = {  \"sequences\" : sequences,\n",
    "                            \"in_columns\" : columns }\n",
    "            out_cols, A = neuroalign(input_dict)\n",
    "            outputs.append(layers.Lambda(lambda x: x, name=\"GPU_\"+str(i)+\"_out_columns\")(out_cols))\n",
    "            outputs.append(layers.Lambda(lambda x: x, name=\"GPU_\"+str(i)+\"_out_attention\")(A))\n",
    "            inputs.extend([sequences, columns])\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    for i, gpu in enumerate(GPUS):\n",
    "        losses_prefixed(losses, metrics, weights, \"GPU_\"+str(i)+\"_\")\n",
    "\n",
    "model.compile(loss=losses, optimizer=optimizer, metrics=metrics, loss_weights=weights)\n",
    "    \n",
    "class ModelCheckpoint(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        neuroalign.save_weights(CHECKPOINT_PATH)\n",
    "        print(\"Saved model to \" + CHECKPOINT_PATH, flush=True)\n",
    "\n",
    "csv_logger = CSVLogger(MODEL_PATH + \"/log.csv\", append=True, separator=',')\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    validation_data=val_gen,\n",
    "                    epochs = NUM_EPOCHS,\n",
    "                    verbose = 1,\n",
    "                    callbacks=[ModelCheckpoint(), csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
