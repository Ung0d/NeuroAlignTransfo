{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Mixture Prior for NeuroAlign\n",
    "\n",
    "#### Problems: \n",
    "- The true probability distribution of a column is only poorly estimated by the relative frequencies $\\frac{n_i}{n}$ where $n_i$ is the count of amino acid $i$ and $n=\\sum_i n_i$, especially if the number of sequences is low (which is the case when training NeuroAlign due to hardware constraints). \n",
    "- If each column predicts its own amino acid distribution, beam search (commonly done in natural language processing) can not be applied when doing autoregressive inference with the model since the output is not expected to agree on a single class (or amino acid).\n",
    "\n",
    "#### Remedy:\n",
    "\n",
    "Model the amino acid distribution of each column by a mixture of Dirichlets (Using Dirichlet mixture priors to derive hidden Markov models for protein families, Brown et al., 1993) i.e. a distribution of the form:\n",
    "\n",
    "$$q_1 P_1 \\dots q_k P_k$$\n",
    "\n",
    "where $P_j$ are Dirichlet densities and $q_j$ are mixture coefficients that sum to 1.\n",
    "\n",
    "Intuitively this models $k$ \"fundamental\" amino acid distributions with the assumption that a particular observed distribution is similar to one of these fundamental distributions. This framework obviously solves the first problem mentioned above. It also solves the second one:\n",
    "\n",
    "Instead of a distribution over the amino acid alphabet, let a model predict a distribution over $k$. If $k$ is sufficiently large, the assumption that every observed count vector is well represented by exactly one of the Dirichlet distributions should be reasonable. Therefore, the model output can be interpreted as a vote for one of $k$ classes and beam search can be implemented by evaluating the $x$ most likely fundamental distributions for the next predicted column at each autoregressive step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_probability==0.11.0\n",
      "  Downloading tensorflow_probability-0.11.0-py2.py3-none-any.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow_probability==0.11.0) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow_probability==0.11.0) (1.18.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from tensorflow_probability==0.11.0) (4.4.2)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.8/site-packages (from tensorflow_probability==0.11.0) (0.1.5)\n",
      "Requirement already satisfied: gast>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow_probability==0.11.0) (0.3.3)\n",
      "Collecting cloudpickle==1.3\n",
      "  Downloading cloudpickle-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Installing collected packages: cloudpickle, tensorflow-probability\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.6.0\n",
      "    Uninstalling cloudpickle-1.6.0:\n",
      "      Successfully uninstalled cloudpickle-1.6.0\n",
      "  Attempting uninstall: tensorflow-probability\n",
      "    Found existing installation: tensorflow-probability 0.12.1\n",
      "    Uninstalling tensorflow-probability-0.12.1:\n",
      "      Successfully uninstalled tensorflow-probability-0.12.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2.30.1 requires cloudpickle>=1.5.0, but you have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
      "Successfully installed cloudpickle-1.3.0 tensorflow-probability-0.11.0\n"
     ]
    }
   ],
   "source": [
    "# uncomment below when using the wolke tf 2.3 image \n",
    "#import sys\n",
    "#!{sys.executable} -m pip install tensorflow_probability==0.11.0\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dirichlet mixture prior is a trainable layer with (alphabet_size + 1) * $k$ weights. It takes as input a batch of count vectors corresponding to the columns of a MSA and the number of sequences of the alignment. It outputs a discrete probability distribution over the $k$ fundamental amino acid distributions for each count vector in the batch.\n",
    "\n",
    "It can be used as follows: In the NeuroAlign model, every time a raw count vector of amino acids appears (inputs and outputs) it is replaced by the output of the DirichletMixturePrior. The weights of the prior itself can:\n",
    "\n",
    "- be pretrained and freezed.\n",
    "- be jointly trained with the model. If $L$ is the model loss, then probably a new loss $L' = \\lambda L + (1-\\lambda) CE(prior, truth)$ can be used.\n",
    "- maybe be found online for some choices of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "alphabet_size = 5\n",
    "\n",
    "class DirichletMixturePrior(layers.Layer):\n",
    "    def __init__(self, k):\n",
    "        super(DirichletMixturePrior, self).__init__()\n",
    "        # Dirichlet parameters > 0\n",
    "        self.alpha = tf.nn.softplus(self.add_weight(shape=(1, k, alphabet_size),\n",
    "                                        name=\"alpha\", initializer=\"uniform\", trainable=True))\n",
    "        # mixture coefficients that sum to 1\n",
    "        self.mixture_coeff = tf.nn.softmax(self.add_weight(shape=(1, k),\n",
    "                                        name=\"mixture_coeff\", initializer=\"uniform\", trainable=True))\n",
    "\n",
    "\n",
    "    # in: n x alphabet_size count vectors \n",
    "    # out: n x k posterior probabilty distribution P(k | count)\n",
    "    def call(self, counts, total_count):\n",
    "        dist = tfp.distributions.DirichletMultinomial(total_count, self.alpha)\n",
    "        probs = dist.prob(tf.expand_dims(counts, 1)) #P(count | p_k)\n",
    "        mix_probs = self.mixture_coeff * probs\n",
    "        return mix_probs / tf.reduce_sum(mix_probs, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "dirichlet_mixture = DirichletMixturePrior(k = 10)\n",
    "\n",
    "# count vectors from a MSA\n",
    "n = 7\n",
    "samples = 8\n",
    "draws = tf.random.categorical([[0.1, 0.3, 0.2, 0.2, 0.2] for _ in range(n)], samples)\n",
    "counts = np.zeros((n, alphabet_size))\n",
    "for i in range(alphabet_size):\n",
    "    counts[:,i] = np.sum(draws == i, axis=-1)\n",
    "\n",
    "start = time.time()\n",
    "count_probs = dirichlet_mixture(counts, samples)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "for i,(p,c) in enumerate(zip(count_probs, counts)):\n",
    "    print(\"count vector: \", i,  c)\n",
    "    print(\"prob: \", i,  p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
